# About
<figure style="float:right; margin-bottom: auto; max-width:35%; min-width:40px;">
    <img src="images/photo51.jpeg" style="border-radius:5%" alt="Photo of me" />
</figure>

[ahmed.khaled@princeton.edu](mailto:ahmed.khaled@princeton.edu)

Welcome to my tiny corner of the internet! I'm Ahmed, I work on optimization and machine learning. I'm a fourth-year Ph.D. student in the ECE department at Princeton University, advised by Prof. [Chi Jin](https://sites.google.com/view/cjin/home). I am interested in optimization in machine learning, and in federated learning.


In the past, I interned at Google DeepMind in 2024 and at Meta AI research in summer 2023. Before that, I interned in the group of Prof. [Peter Richtárik](https://richtarik.org/index.html) at [KAUST](https://www.kaust.edu.sa/en/) in the summers of 2019/2020, where I worked on the distributed & stochastic optimization. Prior to that, I did some research on accelerating the training of neural networks by with Prof. [Amir Atiya](https://scholar.google.com.eg/citations?user=YNxHCMwAAAAJ&hl=en).


# Publications and Preprints

2. [Directional Smoothness and Gradient Methods: Convergence and Adaptivity](https://arxiv.org/abs/2403.04081)  
   NeurIPS 2024, *with [Aaron Mishkin](https://cs.stanford.edu/~amishkin/), [Yuanhao Wang](https://www.cs.princeton.edu/~yuanhao/), [Aaron Defazio](https://www.aarondefazio.com/), and [Robert M. Gower](https://gowerrobert.github.io/)*. [(bibtex)](/static/dirsmooth.bib)


3. [The Road Less Scheduled](https://arxiv.org/abs/2405.15682)  
   NeurIPS 2024 Oral, with [Aaron Defazio](https://www.aarondefazio.com/), [Xingyu (Alice) Yang](https://x.com/alicey_ang), [Harsh Mehta](https://scholar.google.com/citations?user=murJPNoAAAAJ&hl=en), [Konstantin Mishchenko](https://konstmish.com), and [Ashok Cutkosky](https://ashok.cutkosky.com/). [(bibtex)](/static/roadlessscheduled.bib)

6. [Federated Optimization Algorithms with Random Reshuffling and Gradient Compression](https://arxiv.org/abs/2206.07021)  
   NeurIPS 2024, *with [Abdurakhmon Sadiev](https://scholar.google.com/citations?user=R-xZRIAAAAAJ&hl=ru), [Grigory Malinovsky](https://grigory-malinovsky.github.io/), [Eduard Gorbunov](https://eduardgorbunov.github.io/), [Igor Sokolov](https://scholar.google.com/citations?user=OBbPecwAAAAJ&hl=en), [Konstantin Burlachenko](https://burlachenkok.github.io/), and [Peter Richtárik](https://richtarik.org/index.html)*. [(bibtex)](/static/sadiev22fedqrr.bib)

1. [Tuning-Free Stochastic Optimization](https://arxiv.org/abs/2402.07793)  
   ICML 2024 Spotlight, *with [Chi Jin](https://sites.google.com/view/cjin/home)*. [(bibtex)](/static/tuningfree.bib)

3. [DoWG Unleashed: An Efficient Universal Parameter-Free Gradient Descent Method](https://arxiv.org/abs/2305.16284)  
   Advances in Neural Information Processing Systems 35 (NeurIPS 2023), *with [Chi Jin](https://sites.google.com/view/cjin/home) and [Konstantin Mishchenko](https://konstmish.com)*. [(bibtex)](/static/dowg.bib)

4. [Faster federated optimization under second-order similarity](https://arxiv.org/abs/2209.02257)  
   The 11th International Conference on Learning Representations (ICLR 2023), *with [Chi Jin](https://sites.google.com/view/cjin/home)*. [(bibtex)](/static/KJ2022FFSO.bib)

5. [Better Theory for SGD in the Nonconvex World](https://openreview.net/pdf?id=AU4qHN2VkS)  
   Transactions on Machine Learning Research (TMLR) 2023, *with [Peter Richtárik](https://richtarik.org/index.html)*. [(bibtex)](/static/KR2020sgdnonconvex.bib). Original preprint [arXiv:2002.03329](https://arxiv.org/abs/2002.03329) on arXiv since 2020.


7. [Proximal and Federated Random Reshuffling](https://arxiv.org/abs/2102.06704)  
   The 39th International Conference on Machine Learning (ICML 2022), *with [Konstantin Mishchenko](https://konstmish.com) and [Peter Richtárik](https://richtarik.org/index.html)*. [(bibtex)](/static/MKR2021proxrr.bib)

8. [FLIX: A Simple and Communication-Efficient Alternative to Local Methods in Federated Learning](https://arxiv.org/abs/2111.11556)  
   The 25th International Conference on Artificial Intelligence and Statistics (AISTATS 2022), *with [Elnur Gasanov](https://elnurgasanov.com/), [Samuel Horváth](https://samuelhorvath.github.io/), and [Peter Richtárik](https://www.richtarik.org)*. [(bibtex)](/static/GKHR2022flix.bib)

9. [Random Reshuffling: Simple Analysis with Vast Improvements](https://arxiv.org/abs/2006.05988)  
   Advances in Neural Information Processing Systems 33 (NeurIPS 2020), *with [Konstantin Mishchenko](https://konstmish.com) and [Peter Richtárik](https://richtarik.org/index.html)*. [(bibtex)](/static/MKR2020rr.bib)

10. [Tighter Theory for Local SGD on Identical and Heterogeneous Data](https://arxiv.org/abs/1909.04746)  
   The 23rd International Conference on Artificial Intelligence and Statistics (AISTATS) 2020, *with [Konstantin Mishschenko](https://konstmish.com) and [Peter Richtárik](https://richtarik.org/index.html)*. [(bibtex)](/static/KMR2020localsgd.bib). Extends the workshop papers ([a](https://arxiv.org/abs/1909.04746v1), [b](https://arxiv.org/abs/1909.04715))

11. [Unified Analysis of Stochastic Gradient Methods for Composite Convex and Smooth Optimization](https://arxiv.org/abs/2006.11573)  
   Journal version to appear in JOTA 2023, original preprint 2020, *with [Othmane Sebbouh](https://othmanesebbouh.github.io/), [Nicolas Loizou](https://www.maths.ed.ac.uk/~s1461357/), [Robert M. Gower](https://gowerrobert.github.io/), and [Peter Richtárik](https://richtarik.org/index.html)*. [(bibtex)](/static/KSLGR2020unified.bib)

12. [Distributed Fixed Point Methods with Compressed Iterates](https://arxiv.org/abs/1912.09925)  
   Preprint (2019), *with [Sélim Chraibi](https://github.com/Selim78), [Dmitry Kovalev](https://www.dmitry-kovalev.com/), [Peter Richtárik](https://richtarik.org/index.html), [Adil Salim](https://adil-salim.github.io/), and [Martin Takáč](https://mtakac.com/)*. [(bibtex)](/static/CKKRST2019distributed.bib)

13. [Applying Fast Matrix Multiplication to Neural Networks](https://acm.org/doi/abs/10.1145/3341105.3373852)  
   The 35th ACM/SIGAPP Symposium On Applied Computing (ACM SAC) 2020, *with [Amir F. Atiya](https://scholar.google.com.eg/citations?hl=en&user=YNxHCMwAAAAJ) and [Ahmed H. Abdel-Gawad](https://scholar.google.com.eg/citations?user=AbVIlsoAAAAJ&hl=en)*. [(bibtex)](/static/KAA2020fmm.bib)

# Workshop papers

1. [A novel analysis of gradient descent under directional smoothness](https://opt-ml.org/papers/2023/paper77.pdf)  
   5th Annual Workshop on Optimization for Machine Learning (OPT2023), *with [Aaron Mishkin](https://cs.stanford.edu/~amishkin/), [Aaron Defazio](https://www.aarondefazio.com/), and [Robert M. Gower](https://gowerrobert.github.io/)*. [(bibtex)](/static/dir_smoothness.bib)

2. [Better Communication Complexity for Local SGD](https://arxiv.org/abs/1909.04746v1)  
   Oral presentation at the NeurIPS 2019 Workshop on Federated Learning for Data Privacy and Confidentiality, *with [Konstantin Mishschenko](https://konstmish.com) and [Peter Richtárik](https://richtarik.org/index.html)*. [(bibtex)](/static/KMR2019localsgd.bib)

3. [First Analysis of Local GD on Heterogenous Data](https://arxiv.org/abs/1909.04715)  
   NeurIPS 2019 Workshop on Federated Learning for Data Privacy and Confidentiality, *with [Konstantin Mishschenko](https://konstmish.com) and [Peter Richtárik](https://richtarik.org/index.html)*. [(bibtex)](/static/KMR2019localgd.bib)

4. [Gradient Descent with Compressed Iterates](https://arxiv.org/abs/1909.04716)  
   NeurIPS 2019 Workshop on Federated Learning for Data Privacy and Confidentiality, *with [Peter Richtárik](https://richtarik.org/index.html)*. [(bibtex)](/static/KR2019gdci.bib)

# Talks

1. [On the Convergence of Local SGD on Identical and Heterogeneous Data](https://sites.google.com/view/one-world-seminar-series-flow/archive?authuser=0#h.azhfwca3oax9)  
   Federated Learning One World Seminar (2020). [Video](https://www.youtube.com/watch?v=6ThWeKQyp8k&feature=emb_title) and [Slides](/static/FLOW_LocalSGD.pdf)
