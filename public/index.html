<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta name="generator" content="Hugo 0.140.0"><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Ahmed&#39;s corner/</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="robots" content="all,follow">
    <meta name="googlebot" content="index,follow,snippet,archive">
    <link rel="stylesheet" href="http://localhost:1313/hugo-theme-console/css/terminal-0.7.4.min.css">
    <link rel="stylesheet" href="http://localhost:1313/hugo-theme-console/css/animate-4.1.1.min.css">
    <link rel="stylesheet" href="http://localhost:1313/hugo-theme-console/css/console.css">
    
      <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
      <![endif]-->
       
      <link href="http://localhost:1313/index.xml" rel="alternate" type="application/rss+xml" title="Ahmed's corner" />
    <meta property="og:title" content="" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="http://localhost:1313/" />


<meta name="twitter:title" content=""/>
<meta name="twitter:description" content=""/>

<link rel="stylesheet" href="http://localhost:1313/css/custom.css">
</head>
<body class="terminal">
    <div class="container">
        <div class="terminal-nav">
          <header class="terminal-logo">
            <div class="logo terminal-prompt">
              
              
              <a href="http://localhost:1313/" class="no-style ">Ahmed&#39;s corner</a>:~# 
              </div></header>
          <nav class="terminal-menu">
            <ul vocab="https://schema.org/" typeof="BreadcrumbList">
                
                <li><a href="http://localhost:1313/pdfs/cv.pdf" typeof="ListItem">CV</a></li>
                
                <li><a href="https://scholar.google.com/citations?user=Bc3wOdsAAAAJ&amp;hl=en" typeof="ListItem">Scholar</a></li>
                
                <li><a href="https://github.com/rka97" typeof="ListItem">GitHub</a></li>
                
            </ul>
          </nav>
        </div>
    </div>

    <div class="container animated zoomIn fast" >
        
  <h1 id="about">About</h1>
<figure style="float:right; margin-bottom: auto; max-width:35%; min-width:40px;">
    <img src="images/photo51.jpeg" style="border-radius:5%" alt="Photo of me" />
</figure>
<p><a href="mailto:ahmed.khaled@princeton.edu">ahmed.khaled@princeton.edu</a></p>
<p>Welcome to my tiny corner of the internet! I&rsquo;m Ahmed, I work on optimization and machine learning. I&rsquo;m a fourth-year Ph.D. student in the ECE department at Princeton University, advised by Prof. <a href="https://sites.google.com/view/cjin/home">Chi Jin</a>. I am interested in optimization in machine learning, and in federated learning.</p>
<p>In the past, I interned at Google DeepMind in 2024 and at Meta AI research in summer 2023. Before that, I interned in the group of Prof. <a href="https://richtarik.org/index.html">Peter Richtárik</a> at <a href="https://www.kaust.edu.sa/en/">KAUST</a> in the summers of 2019/2020, where I worked on the distributed &amp; stochastic optimization. Prior to that, I did some research on accelerating the training of neural networks by with Prof. <a href="https://scholar.google.com.eg/citations?user=YNxHCMwAAAAJ&amp;hl=en">Amir Atiya</a>.</p>
<h1 id="publications-and-preprints">Publications and Preprints</h1>
<ol start="2">
<li>
<p><a href="https://arxiv.org/abs/2403.04081">Directional Smoothness and Gradient Methods: Convergence and Adaptivity</a><br>
NeurIPS 2024, <em>with <a href="https://cs.stanford.edu/~amishkin/">Aaron Mishkin</a>, <a href="https://www.cs.princeton.edu/~yuanhao/">Yuanhao Wang</a>, <a href="https://www.aarondefazio.com/">Aaron Defazio</a>, and <a href="https://gowerrobert.github.io/">Robert M. Gower</a></em>. <a href="/static/dirsmooth.bib">(bibtex)</a></p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2405.15682">The Road Less Scheduled</a><br>
NeurIPS 2024 Oral, with <a href="https://www.aarondefazio.com/">Aaron Defazio</a>, <a href="https://x.com/alicey_ang">Xingyu (Alice) Yang</a>, <a href="https://scholar.google.com/citations?user=murJPNoAAAAJ&amp;hl=en">Harsh Mehta</a>, <a href="https://konstmish.com">Konstantin Mishchenko</a>, and <a href="https://ashok.cutkosky.com/">Ashok Cutkosky</a>. <a href="/static/roadlessscheduled.bib">(bibtex)</a></p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2206.07021">Federated Optimization Algorithms with Random Reshuffling and Gradient Compression</a><br>
NeurIPS 2024, <em>with <a href="https://scholar.google.com/citations?user=R-xZRIAAAAAJ&amp;hl=ru">Abdurakhmon Sadiev</a>, <a href="https://grigory-malinovsky.github.io/">Grigory Malinovsky</a>, <a href="https://eduardgorbunov.github.io/">Eduard Gorbunov</a>, <a href="https://scholar.google.com/citations?user=OBbPecwAAAAJ&amp;hl=en">Igor Sokolov</a>, <a href="https://burlachenkok.github.io/">Konstantin Burlachenko</a>, and <a href="https://richtarik.org/index.html">Peter Richtárik</a></em>. <a href="/static/sadiev22fedqrr.bib">(bibtex)</a></p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2402.07793">Tuning-Free Stochastic Optimization</a><br>
ICML 2024 Spotlight, <em>with <a href="https://sites.google.com/view/cjin/home">Chi Jin</a></em>. <a href="/static/tuningfree.bib">(bibtex)</a></p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2305.16284">DoWG Unleashed: An Efficient Universal Parameter-Free Gradient Descent Method</a><br>
Advances in Neural Information Processing Systems 35 (NeurIPS 2023), <em>with <a href="https://sites.google.com/view/cjin/home">Chi Jin</a> and <a href="https://konstmish.com">Konstantin Mishchenko</a></em>. <a href="/static/dowg.bib">(bibtex)</a></p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2209.02257">Faster federated optimization under second-order similarity</a><br>
The 11th International Conference on Learning Representations (ICLR 2023), <em>with <a href="https://sites.google.com/view/cjin/home">Chi Jin</a></em>. <a href="/static/KJ2022FFSO.bib">(bibtex)</a></p>
</li>
<li>
<p><a href="https://openreview.net/pdf?id=AU4qHN2VkS">Better Theory for SGD in the Nonconvex World</a><br>
Transactions on Machine Learning Research (TMLR) 2023, <em>with <a href="https://richtarik.org/index.html">Peter Richtárik</a></em>. <a href="/static/KR2020sgdnonconvex.bib">(bibtex)</a>. Original preprint <a href="https://arxiv.org/abs/2002.03329">arXiv:2002.03329</a> on arXiv since 2020.</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2102.06704">Proximal and Federated Random Reshuffling</a><br>
The 39th International Conference on Machine Learning (ICML 2022), <em>with <a href="https://konstmish.com">Konstantin Mishchenko</a> and <a href="https://richtarik.org/index.html">Peter Richtárik</a></em>. <a href="/static/MKR2021proxrr.bib">(bibtex)</a></p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2111.11556">FLIX: A Simple and Communication-Efficient Alternative to Local Methods in Federated Learning</a><br>
The 25th International Conference on Artificial Intelligence and Statistics (AISTATS 2022), <em>with <a href="https://elnurgasanov.com/">Elnur Gasanov</a>, <a href="https://samuelhorvath.github.io/">Samuel Horváth</a>, and <a href="https://www.richtarik.org">Peter Richtárik</a></em>. <a href="/static/GKHR2022flix.bib">(bibtex)</a></p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2006.05988">Random Reshuffling: Simple Analysis with Vast Improvements</a><br>
Advances in Neural Information Processing Systems 33 (NeurIPS 2020), <em>with <a href="https://konstmish.com">Konstantin Mishchenko</a> and <a href="https://richtarik.org/index.html">Peter Richtárik</a></em>. <a href="/static/MKR2020rr.bib">(bibtex)</a></p>
</li>
<li>
<p><a href="https://arxiv.org/abs/1909.04746">Tighter Theory for Local SGD on Identical and Heterogeneous Data</a><br>
The 23rd International Conference on Artificial Intelligence and Statistics (AISTATS) 2020, <em>with <a href="https://konstmish.com">Konstantin Mishschenko</a> and <a href="https://richtarik.org/index.html">Peter Richtárik</a></em>. <a href="/static/KMR2020localsgd.bib">(bibtex)</a>. Extends the workshop papers (<a href="https://arxiv.org/abs/1909.04746v1">a</a>, <a href="https://arxiv.org/abs/1909.04715">b</a>)</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2006.11573">Unified Analysis of Stochastic Gradient Methods for Composite Convex and Smooth Optimization</a><br>
Journal version to appear in JOTA 2023, original preprint 2020, <em>with <a href="https://othmanesebbouh.github.io/">Othmane Sebbouh</a>, <a href="https://www.maths.ed.ac.uk/~s1461357/">Nicolas Loizou</a>, <a href="https://gowerrobert.github.io/">Robert M. Gower</a>, and <a href="https://richtarik.org/index.html">Peter Richtárik</a></em>. <a href="/static/KSLGR2020unified.bib">(bibtex)</a></p>
</li>
<li>
<p><a href="https://arxiv.org/abs/1912.09925">Distributed Fixed Point Methods with Compressed Iterates</a><br>
Preprint (2019), <em>with <a href="https://github.com/Selim78">Sélim Chraibi</a>, <a href="https://www.dmitry-kovalev.com/">Dmitry Kovalev</a>, <a href="https://richtarik.org/index.html">Peter Richtárik</a>, <a href="https://adil-salim.github.io/">Adil Salim</a>, and <a href="https://mtakac.com/">Martin Takáč</a></em>. <a href="/static/CKKRST2019distributed.bib">(bibtex)</a></p>
</li>
<li>
<p><a href="https://acm.org/doi/abs/10.1145/3341105.3373852">Applying Fast Matrix Multiplication to Neural Networks</a><br>
The 35th ACM/SIGAPP Symposium On Applied Computing (ACM SAC) 2020, <em>with <a href="https://scholar.google.com.eg/citations?hl=en&amp;user=YNxHCMwAAAAJ">Amir F. Atiya</a> and <a href="https://scholar.google.com.eg/citations?user=AbVIlsoAAAAJ&amp;hl=en">Ahmed H. Abdel-Gawad</a></em>. <a href="/static/KAA2020fmm.bib">(bibtex)</a></p>
</li>
</ol>
<h1 id="workshop-papers">Workshop papers</h1>
<ol>
<li>
<p><a href="https://opt-ml.org/papers/2023/paper77.pdf">A novel analysis of gradient descent under directional smoothness</a><br>
5th Annual Workshop on Optimization for Machine Learning (OPT2023), <em>with <a href="https://cs.stanford.edu/~amishkin/">Aaron Mishkin</a>, <a href="https://www.aarondefazio.com/">Aaron Defazio</a>, and <a href="https://gowerrobert.github.io/">Robert M. Gower</a></em>. <a href="/static/dir_smoothness.bib">(bibtex)</a></p>
</li>
<li>
<p><a href="https://arxiv.org/abs/1909.04746v1">Better Communication Complexity for Local SGD</a><br>
Oral presentation at the NeurIPS 2019 Workshop on Federated Learning for Data Privacy and Confidentiality, <em>with <a href="https://konstmish.com">Konstantin Mishschenko</a> and <a href="https://richtarik.org/index.html">Peter Richtárik</a></em>. <a href="/static/KMR2019localsgd.bib">(bibtex)</a></p>
</li>
<li>
<p><a href="https://arxiv.org/abs/1909.04715">First Analysis of Local GD on Heterogenous Data</a><br>
NeurIPS 2019 Workshop on Federated Learning for Data Privacy and Confidentiality, <em>with <a href="https://konstmish.com">Konstantin Mishschenko</a> and <a href="https://richtarik.org/index.html">Peter Richtárik</a></em>. <a href="/static/KMR2019localgd.bib">(bibtex)</a></p>
</li>
<li>
<p><a href="https://arxiv.org/abs/1909.04716">Gradient Descent with Compressed Iterates</a><br>
NeurIPS 2019 Workshop on Federated Learning for Data Privacy and Confidentiality, <em>with <a href="https://richtarik.org/index.html">Peter Richtárik</a></em>. <a href="/static/KR2019gdci.bib">(bibtex)</a></p>
</li>
</ol>
<h1 id="talks">Talks</h1>
<ol>
<li><a href="https://sites.google.com/view/one-world-seminar-series-flow/archive?authuser=0#h.azhfwca3oax9">On the Convergence of Local SGD on Identical and Heterogeneous Data</a><br>
Federated Learning One World Seminar (2020). <a href="https://www.youtube.com/watch?v=6ThWeKQyp8k&amp;feature=emb_title">Video</a> and <a href="/static/FLOW_LocalSGD.pdf">Slides</a></li>
</ol>


        <div class="footer">
    Powered by <a href="https://gohugo.io/">Hugo</a> with
    <a href="https://github.com/mrmierzejewski/hugo-theme-console/">Console Theme</a>. 
</div>

    </div>
  </body>
</html>
