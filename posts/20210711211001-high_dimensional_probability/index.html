<!DOCTYPE html>
<html lang="en-us">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Ahmed&#39;s corner/posts/20210711211001-high_dimensional_probability/</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="robots" content="all,follow">
    <meta name="googlebot" content="index,follow,snippet,archive">
    <link rel="stylesheet" href="https://rka97.github.io/hugo-theme-console/css/terminal-0.7.1.min.css">
    <link rel="stylesheet" href="https://rka97.github.io/hugo-theme-console/css/animate-3.7.2.min.css">
    <link rel="stylesheet" href="https://rka97.github.io/hugo-theme-console/css/console.css">
    <link rel="stylesheet" href="https://rka97.github.io/hugo-theme-console/css/theorems.css">
    
      <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
      <![endif]-->
       <meta property="og:title" content="High-dimensional probability" />
<meta property="og:description" content="" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://rka97.github.io/posts/20210711211001-high_dimensional_probability/" />


<meta name="twitter:title" content="High-dimensional probability"/>
<meta name="twitter:description" content="This worksheet contains my literature notes and solutions for (Vershynin 2018).
 Chapter 1: Preliminaries on random variable 1.1: Basic quantities associated with random variables In this section we recall some basic quantities associated with random variables.
 We denote by \(\ec{X}\) the mean of \(X\) and by \(\Var(X) \eqdef \ecn{X - \ec{X}}\) the variance of \(X\).
 A quantity that describes probability distributions is the moment-generating function of \(X\), defined as:"/>

<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-160614739-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

<script>
  MathJax = {
  loader: {load: ['[tex]/tagformat']},
  tex: {
  inlineMath: [['$', '$'], ['\\(', '\\)']],
  displayMath: [['$$','$$'], ['\\[', '\\]']],
  processEscapes: true,
  processEnvironments: true,
  tags: 'ams',
  packages: {'[+]': ['tagformat']},
  tagformat: {
    number: (n) =>  n,
    id: (label) => label
    },
  macros: {
    br: ["\\left ( #1 \\right )", 1],
    Var: ["\\mathrm{Var}"],
    norm: ["\\left\\lVert#1\\right\\rVert", 1],
    sqn: ["{\\left\\lVert#1\\right\\rVert}^2", 1],
    abs: ["\\left\\lvert#1\\right\\rvert", 1],
    ec: ["\\mathbb{E} \\left \[ #1 \\right \]", 1],
    ecn: ["\\mathbb{E} \\left \[ \\| #1 \\|^2 \\right \]", 1],
    eca: ["\\mathbb{E} \\left \[ \\left \\lvert #1 \\right \\rvert \\right \]", 1],
    pr: ["\\\mathrm{Prob}\\left(#1\\right)", 1],
    ev: ["\\left \\langle #1 \\right \\rangle", 1],
    br: ["\\left ( #1 \\right )", 1],
    pbr: ["\\left \\{ #1 \\right \\} ", 1],
    floor: ["\\left \\lfloor #1 \\right \\rfloor", 1],
    dbtilde: ["\\accentset{\\approx}{#1}", 1],
    N: ["\\mathbb{N}"],
    Z: ["\\mathbb{Z}"],
    Q: ["\\mathbb{Q}"],
    R: ["\\mathbb{R}"],
    X: ["\\mathcal{X}"],
    E: ["\\mathbb{E}"],
    Snp: ["\\mathbb{S}^{n}_{+}"],
    Sn: ["\\mathbb{S}^{n}"],
    F: ["\\mathcal{F}"],
    A: ["\\mathcal{A}"],
    B: ["\\beta"],
    J: ["\\mathcal{J}"],
    G: ["\\mathcal{G}"],
    lc: ["\\mathop l"],
    C: ["\\mathbb{C}"],
    K: ["\\mathcal{K}"],
    lub: ["\\mathrm{lub}"],
    g: ["\\mathrm{glb}"],
    seq: ["\\subseteq"],
    e: ["\\varepsilon"],
    la: ["\\lambda"],
    om: ["\\omega"],
    Om: ["\\Omega"],
    de: ["\\delta"],
    mbf: ["\\mathbf"],
    es: ["\\emptyset"],
    mc: ["\\mathcal"],
    un: ["\\cup"],
    ic: ["\\cap"],
    spn: ["\\mathrm{span \\ }"],
    dm: ["\\mathrm{dim \\ }"],
    sgn: ["\\mathrm{ \ sign}"],
    Lm: ["\\mathcal{L}"],
    nll: ["\\mathrm{null}"],
    diag: ["\\mathrm{diag }"],
    row: ["\\mathrm{row}"],
    col: ["\\mathrm{col \\ }"],
    rng: ["\\mathrm{range \\ }"],
    dgr: ["\\mathrm{deg \\ }"],
    dist: ["\\mathrm{dist}"],
    Prob: ["\\mathrm{Prob}"],
    Lim: ["\\lim\limits"],
    Sum: ["\\sum\limits"],
    Pt: ["\\|P\\|"],
    dmn: ["\\mathrm{dom \\ }"],
    Prod: ["\\prod\limits"],
    Beta: ["\\beta"],
    Seq: ["\\mathrm{Seq }"],
    adj: ["\\mathrm{adj \\ }"],
    rank: ["\\mathrm{rank \\ }"],
    epi: ["\\mathrm{epi}"],
    tr: ["\\mathrm{tr}"],
    op: ["\\mathrm{op}"],
    prox: ["\\mathrm{prox}"],
    D: ["\\mathcal{D}"],
    eqdef: ["\\overset{\\text{def}}{=}"],
    var: ["\\mathrm{Var}"],
    cov: ["\\mathrm{cov}"],
    Var: ["\\mathrm{Var}"],
    Ber: ["\\mathrm{Ber}"],
    Bern: ["\\mathrm{Bern}"],
    tvar: ["\\mathrm{TVar}"],
    Binom: ["\\mathrm{Binom}"],
    Pois: ["\\mathrm{Pois}"],
    cO: ["\\mathcal{O}"]
    },

  },
  options:
  {
  skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
  };

  window.addEventListener('load', (event) => {
    document.querySelectorAll("mjx-container").forEach(function(x){
    x.parentElement.classList += 'has-jax'})
  });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link rel="stylesheet" href="https://rka97.github.io/hugo-theme-console/css/theorems.css">

</head>
<body class="terminal">
    <div class="container">
        <div class="terminal-nav">
          <header class="terminal-logo">
            <div class="logo terminal-prompt">
              
              
              <a href="https://rka97.github.io" class="no-style site-name">Ahmed&#39;s corner</a>:~# 
              <a href='https://rka97.github.ioposts'>posts</a>/<a href='https://rka97.github.ioposts/20210711211001-high_dimensional_probability'>20210711211001-high_dimensional_probability</a>/</div></header>
          <nav class="terminal-menu">
            <ul vocab="https://schema.org/" typeof="BreadcrumbList">
                
                <li><a href="https://rka97.github.io/pdfs/cv.pdf" typeof="ListItem">CV</a></li>
                
                <li><a href="https://scholar.google.com/citations?user=Bc3wOdsAAAAJ&amp;hl=en" typeof="ListItem">Scholar</a></li>
                
                <li><a href="https://github.com/rka97" typeof="ListItem">GitHub</a></li>
                
            </ul>
          </nav>
        </div>
    </div>

    <div class="container " >
        
<h1>High-dimensional probability</h1>

Jan. 1, 0001


<br/><br/>
<div class="abstract">
  <div></div>
<p>This worksheet contains my literature notes and solutions for (<a href="#orgb08bfa8">Vershynin 2018</a>).</p>
</div>
<h2 id="chapter-1-preliminaries-on-random-variable">Chapter 1: Preliminaries on random variable</h2>
<h3 id="1-dot-1-basic-quantities-associated-with-random-variables">1.1: Basic quantities associated with random variables</h3>
<p>In this section we recall some basic quantities associated with random variables.</p>
<div class="definition" id="def:exp-and-variance">
  <div></div>
<p>We denote by \(\ec{X}\) the mean of \(X\) and by \(\Var(X) \eqdef \ecn{X - \ec{X}}\) the <em>variance</em> of \(X\).</p>
</div>
<p>A quantity that describes probability distributions is the <em>moment-generating function</em> of \(X\), defined as:</p>
<div class="definition" id="def:moment-generating-function">
  <div></div>
<p>We define the moment generating function of \(X\) as:
\[ M_X(t) \eqdef \ec{e^{tX}}, \text { for  } t \in \R. \]</p>
</div>
<p>We can equip probability spaces with various norms: the \(L^p\) norms in particular will be handy:</p>
<div class="definition" id="def:Lp-norm">
  <div></div>
<p>We define the \(p\) th moment of \(X\) as \(\ec{X^p}\), and the \(p\) th absolute moment is \(\ec{\norm{X}^p}\). We define the \(L^p\) norm of a random variable \(X\) as:
\[ \norm{X}_{L^p} = \left ( \ec{\norm{X}^p} \right )^{1/p}. \]
We can extend this definition to \(p = \infty\) using the essential supremum of \(\abs{X}\):
\[ \norm{X}_{L^{\infty}} = \mathrm{ess sup} \norm{X}. \]
where essential supremum is just the almost-everywhere (i.e. with probability \(1\)) supremum.</p>
</div>
<p>We define the \(L^p\) space as the space consisting of all random variables \(X\) on \(\Omega\) with finite \(L^p\) norm. If \(p \in [1, \infty)\), the quantity \(\norm{X}_{L^p}\) is a norm and \(L^p\) is a <em>Banach space</em>. This fact follows from Minkowsky&rsquo;s inequality (see Proposition <a href="#prop:minkowski-inequality">prop:minkowski-inequality</a>). For \(p &lt; 1\), \(\norm{X}_{L^p}\) is not a norm. The \(L^2\) space is special, because it is also a Hilbert space: the inner product and corresponding norm on \(L^2\) are given by
\[ \ev{X, Y}_{L^2} = \ec{XY}, \ \norm{X}_{L^2} = \left ( \ecn{X} \right )^{1/2}. \]
The <em>standard deviation</em> of \(X\) can be expressed as</p>
<p>\[ \norm{X - \ec{X}}_{L^2} = \sqrt{\Var(X)} = \sigma(X). \]</p>
<p>And the <em>covariance</em> of the random variables \(X\) and \(Y\) as
\[ \cov(X, Y) \eqdef \ec{(X-\ec{X}) (Y - \ec{Y})} = \ev{X - \ec{X}, Y - \ec{Y}}_{L^2}, \]</p>
<h3 id="1-dot-2-some-classical-inequalities">1.2: Some classical inequalities</h3>
<div class="proposition" id="prop:jensen-inequality">
  <div></div>
<p><strong>Jensen&rsquo;s inequality</strong> states that for any random variable \(X\) and a convex function \(\phi: \R \to \R\), we have</p>
<p>\begin{equation}
\label{eq:jensen}
\phi(\ec{X}) \leq \ec{\phi{X}}.
\end{equation}</p>
</div>
<div class="proof">
  <div></div>
<p>This is a trivial consequence of the definition of convexity.</p>
</div>
<p>Applying \eqref{eq:jensen} to \(\phi(x) = x^{q/p}\) (which is convex if \(q/p \geq 1\)), we get that \(\norm{X}_{L^p}\) is an increasing function in \(p\):</p>
<p>\begin{equation}
\norm{X}_{L^p} \leq \norm{X}_{L^q}, \text { for any  } 0 \leq p \leq q.
\end{equation}</p>
<div class="proposition" id="prop:minkowski-inequality">
  <div></div>
<p><strong>Minkowski&rsquo;s inequality</strong> states that for any \(p \in [1, \infty]\) and any random variables \(X, Y \in L^p\), we have
\[ \norm{X+Y}_{L^p} \leq \norm{X}_{L^p} + \norm{Y}_{L^p}. \]
This is also termed the <em>triangle inequality</em>.</p>
</div>
<div class="proof">
  <div></div>
<p>FIXME.</p>
</div>
<div class="proposition" id="prop:cauchy-schwarz">
  <div></div>
<p>The <strong>Cauchy-Schwarz</strong> inequality states that for any random variables \(X, Y \in L^2\), we have
\[ \abs{\ec{XY}} \leq \norm{X}_{L^2} \norm{Y}_{L^2}. \]</p>
</div>
<p>A generalization of Proposition <a href="#prop:cauchy-schwarz">prop:cauchy-schwarz</a> is Hölder&rsquo;s inequality:</p>
<div class="proposition" id="prop:holder-inequality">
  <div></div>
<p>Hölder&rsquo;s inequality states that if \(p, q \in (1, \infty)\) are conjugate exponents (i.e. \(1/p + 1/q = 1\)), then
\[ \abs{\ec{XY}} \leq \ec{X}_{L^p} \ec{Y}_{L^q}, \]
where \(X \in L^p\) and \(Y \in L^q\). This inequality also holds for the pair \(p = 1, q = \infty\).</p>
</div>
<div class="definition" id="def:cdf-and-tail">
  <div></div>
<p>We define the cumulative distribution function (CDF) of \(X\) as
\[ F_X(t) = \pr{X \leq t}. \]
We define the <em>tail</em> of a random variable \(X\) as
\[ \pr{X &gt; t} = 1 - F_X(t). \]</p>
</div>
<div class="lemma" id="lemma:integral-identity">
  <div></div>
<p>Let \(X\) be a nonnegative random variable. Then,
\[ \ec{X} = \int_0^{\infty} \pr{X &gt; t} dt. \]
The two sides of this identity are either finite or infinite simultaneously.</p>
</div>
<div class="proof">
  <div></div>
<p>We can represent any nonnegative real number \(x\) via the identity
\[ x = \int_0^x 1 dt = \int_0^{\infty} 1_{t \leq x} dt. \]
Substitute the random variable \(X\) for \(x\) and take expectations of both sides:</p>
<p>\begin{equation}
\ec{X} = \ec{\int_0^{\infty} 1_{t &lt; X} dt} = \int_0^{\infty} \ec{1_{t &lt; X}} dt = \int_0^{\infty} \pr{X &gt; t} dt,
\end{equation}</p>
<p>where the exchange of order is permitted by some Fubini-Tonelli&rsquo;s theorem from measure theory. I don&rsquo;t care, my variables are mostly finite.</p>
</div>
<div class="exercise">
  <div></div>
<p>Prove the following generalization of Lemma <a href="#lemma:integral-identity">lemma:integral-identity</a>, which is valid for any random variable \(X\) (not necessarily nonnegative):
\[ \ec{X} = \int_0^{\infty} \pr{X &gt; t} dt - \int_{-\infty}^0 \pr{X &lt; t} dt. \]</p>
</div>
<div class="proof">
  <div></div>
<p>Any real number \(x\) can be written as
\[ x = 1_{x \geq 0} x - 1_{x &lt; 0} (-x). \]
Taking expectation we have</p>
<p>\begin{equation}
\label{eq:exr122-1}
\ec{X} = \ec{ X 1_{X \geq 0} } - \ec{ (-X) 1_{X &lt; 0}  }.
\end{equation}</p>
<p>Using Lemma <a href="#lemma:integral-identity">lemma:integral-identity</a>, we can write the first term in \eqref{eq:exr122-1} as
\[ \ec{X 1_{X \geq 0}} = \int_0^{\infty} \pr(1_{X \geq 0} X &gt; t) dt = \int_0^{\infty} \pr(X &gt; t) dt, \]
where we used that \(t \in (0, \infty)\). The second term can be written as</p>
<p>\begin{align}
\ec{(-X) 1_{X &lt; 0}} &amp;= \int_0^{\infty} \pr{(-X) 1_{X &lt; 0} &gt; t} dt \\\<br>
&amp;= \int_0^{\infty} \pr{X 1_{X &lt; 0} &lt; -t} dt \\\<br>
&amp;= \int_{-\infty}^{0} \pr{X 1_{X &lt; 0} &lt; t} dt \\\<br>
&amp;= \int_{-\infty}^0 \pr{X &lt; t} dt,
\end{align}</p>
<p>where we applied a change of variables \(t \to -t\) in the third line.</p>
</div>
<div class="exercise">
  <div></div>
<p>Let \(X\) be a random variable and \(p \in (0, \infty)\). Show that
\[ \ec{\abs{X}^p} = \int_0^{\infty} p t^{p-1} \pr{\abs{X} &gt; t} dt. \]</p>
</div>
<div class="proof">
  <div></div>
<p>Using the integral identity (Lemma <a href="#lemma:integral-identity">lemma:integral-identity</a>) for \(\abs{X}^p\) we have
\[ \ec{\abs{X}^p} = \int_0^{\infty} \pr{\abs{X}^p &gt; t} dt = \int_0^{\infty} \pr{\abs{X} &gt; t^{1/p}} dt. \]
Applying the variable change \(t \to z^p\), we have
\[ \ec{X} = \int_0^{\infty} \pr{\abs{X} &gt; z} p z^{p-1} dz, \]
and this is the required identity.</p>
</div>
<p>The next inequality is important in concentration arguments:</p>
<div class="proposition" id="prop:markov-inequality">
  <div></div>
<p>For any nonnegative random variable \(X\) and \(t &gt; 0\), we have
\[ \pr{X \geq t} \leq \frac{\ec{X}}{t}. \]</p>
</div>
<div class="proof">
  <div></div>
<p>Fix \(t &gt; 0\). We can represent any real number \(x\) via the identity
\[ x = x 1_{x \geq t} + x 1_{x \leq t}. \]
Taking expectations,</p>
<p>\begin{align}
\ec{X} &amp;= \ec{X 1_{X \geq t}} + \ec{X 1_{X &lt; t}} \\\<br>
&amp;\geq \ec{t 1_{X \geq t}} = t \cdot \pr{X \geq t}.
\end{align}</p>
<p>Dividing both sides by \(t\), we complete the proof.</p>
</div>
<div class="proposition" id="prop:chebychev-inequality">
  <div></div>
<p>Let \(X\) be a random variable with mean \(\mu\) and variance \(\sigma^2\). Then for any \(t &gt; 0\), we have
\[ \pr{\abs{X - \mu} \geq t} \leq \frac{\sigma^2}{t^2}. \]</p>
</div>
<div class="proof">
  <div></div>
<p>Note that \(\sqn{X - \mu}\) is a nonnegative random variable with mean \(\sigma^2\). Hence, using Markov&rsquo;s inequality we have
\[ \pr{\abs{X - \mu}^2 \geq t^2} \leq \frac{\ecn{X - \mu}}{t^2}. \]
Using the definition of \(\sigma^2\), we have
\[ \pr{\abs{X - \mu} \geq t} \leq \frac{\sigma^2}{t^2}. \]</p>
</div>
<div class="exercise">
  <div></div>
<p>Deduce Chebyshev&rsquo;s inequality by squaring both sides of the bound \(\norm{X - \mu} \geq t\) and applying Markov&rsquo;s inequality.</p>
</div>
<div class="proof">
  <div></div>
<p>See above.</p>
</div>
<h3 id="1-dot-3-limit-theorems">1.3: Limit theorems</h3>
<div class="theorem" id="thm:strong-law-of-large-numbers">
  <div></div>
<p>(The strong law of large numbers). Let \(X_1, X_2, \ldots\) be a sequence of i.i.d. random variables with mean \(\mu\). Consider the sum
\[ S_N = X_1 + \ldots + X_N. \]
Then, as \(N \to \infty\),
\[ \frac{S_N}{N} \to \mu, \text { almost surely. } \]</p>
</div>
<div class="theorem" id="thm:lindeberg-levy-central-limit-theorem">
  <div></div>
<p>(Lindeberg-Lévy central limit theorem). Let \(X_1, X_2, \ldots\) be a sequence of i.i.d. random variables with mean \(\mu\) and variance \(\sigma^2\). Consider the sum
\[ S_N = X_1 + \ldots + X_N \]
and normalize it to obtain a random variable with zero mean and unit variance as follows:
\[ Z_N \eqdef \frac{S_N - \ec{S_N}}{\sqrt{\Var(S_N)}} = \frac{1}{\sigma \sqrt{N}} \sum_{i=1}^N (X_i - \mu). \]
Then, as \(N \to \infty\),
\[ Z_N \to N(0, 1) \text { in distribution. } \]
Convergence in distribution means that the CDF of \(Z_N\) converges pointwise to the CDF of the standard normal distribution. In terms of tails, then for every \(t \in \R\), we have
\[ \pr{Z_N \geq t} \to \pr{g \geq t} = \frac{1}{\sqrt{2 \pi}} \int_t^{\infty} e^{-x^2/2} dx \]
as \(N \to \infty\), where \(g \sim N(0, 1)\) is a standard normal random variable.</p>
</div>
<div class="exercise">
  <div></div>
<p>Let \(X_1, X_2, \ldots\) be a sequence of i.i.d. random variables with mean \(\mu\) and finite variance. Show that
\[ \ec{ \abs{\frac{1}{N} \sum_{i=1}^N X_i - \mu  }} = \mathcal{O}\br{\frac{1}{\sqrt{N}}}, \]
as \(N \to \infty\).</p>
</div>
<div class="proof">
  <div></div>
<p>Since \(\norm{X}_{L^p}\) is an increasing function of \(p\), we have</p>
<p>\begin{equation}
\label{eq:exc133-1}
\norm{ \frac{S_N}{N} - \mu }_{L^1} \leq \norm{ \frac{S_N}{N} - \mu }_{L^2}.
\end{equation}</p>
<p>The left hand side is simply
\[ \norm{ \frac{S_N}{N} - \mu  }_{L^1} = \ec{\abs{\frac{1}{N} \sum_{i=1}^N X_i - \mu}}. \]
While the right hand side of \eqref{eq:exc133-1} can be bounded as
\[ \norm{\frac{S_N}{N} - \mu}_{L^2} = \sqrt{ \ec{\abs{\frac{S_N}{N} - \mu}^2} }, \]
Using \(Z_N\) from Theorem <a href="#thm:lindeberg-levy-central-limit-theorem">thm:lindeberg-levy-central-limit-theorem</a>, we have
\[ \ecn{Z_N} \to 1 \text { as } N \to \infty. \]
Hence,
\[ \frac{1}{\sigma^2} \eca{ \frac{1}{\sqrt{N}} \sum_{i=1}^N (X_i - \mu)} \to 1 \text { as  } N \to \infty. \]
Equivalently we have,
\[ \frac{1}{\sigma^2} \eca{\frac{1}{N}\sum_{i=1}^N (X_i - \mu)} \to \frac{1}{N} \text { as } N \to \infty. \]
It remains to take the square root of both sides and use Jensen&rsquo;s inequality \eqref{eq:jensen}.</p>
</div>
<div class="theorem" id="thm:demoivre-laplace-theorem">
  <div></div>
<p>If the \(X_i\) are Bernoulli random variables with some fixed parameter \(p\), denoted
\[ X_i \sim \Ber(p). \]
The sum \(S_N = X_1 + \ldots + X_N\) is said to have the <em>binomial distribution</em> \(\Binom(N, p)\). The central limit theorem yields that as \(N \to \infty\),
\[ \frac{S_N - N p}{\sqrt{N p (1-p)}} \to N(0, 1) \text { in distribution. } \]
This special case is known as the <em>de Moivre-Laplace theorem</em>.</p>
</div>
<div class="definition" id="def:poisson-distribution">
  <div></div>
<p>We say that a random variable \(Z\) has <em>Poisson distribution</em> with parameter \(\lambda\), denoted \(Z \sim \Pois(\lambda)\), if it takes values in \(\{ 0, 1, 2, \ldots \}\) with probabilities
\[ \pr{Z=k} = \exp(-\lambda) \frac{\lambda^k}{k!}, \text { for  } k = 0, 1, 2, \ldots \]</p>
</div>
<div class="theorem" id="thm:poisson-limit-theorem">
  <div></div>
<p>Let \(X_{N, i}\), \(1 \leq i \leq N\), be independent random variables \(X_{N, i} \sim \Ber(p_{N, i})\), and let \(S_N = \sum_{i=1}^N X_{N, i}\). Assume that, as \(N \to \infty\),
\[ \max_{i \leq N} p_{N, i} \to 0, \text { and } \ec{S_N} = \sum_{i=1}^N p_{N, i} \to \lambda &lt; \infty. \]
Then, as \(N \to \infty\),
\[ S_N \to \Pois(\lambda) \text { in distribution. } \]</p>
</div>
<h2 id="chapter-2-concentration-of-sums-of-independent-random-variables">Chapter 2: Concentration of sums of independent random variables</h2>
<h3 id="2-dot-1-why-concentration-inequalities">2.1: Why concentration inequalities?</h3>
<div class="exercise">
  <div></div>
<p>Toss a fair coin \(N\) times. What is the probability that we get at least \(\frac{3N}{4}\)?</p>
</div>
<div class="proof">
  <div></div>
<p>Let \(S_N\) denote the number of heads. Then using Proposition <a href="#prop:chebychev-inequality">prop:chebychev-inequality</a>, the probability of getting at least \(\frac{3}{4} N\) heads is bounded as:
\[ \pr{S_N \geq \frac{3}{4}N} \leq \pr{\abs{S_N - \frac{N}{2}}} \leq \frac{4}{N}. \]
So the probability converges to zero at least <em>linearly</em> in \(N\).</p>
</div>
<div class="proposition" id="prop:tails-of-normal-distribution">
  <div></div>
<p>Let \(g \sim N(0, 1)\). Then for all \(t &gt; 0\), we have
\[ \br{\frac{1}{t} - \frac{1}{t^3}} \cdot \frac{1}{\sqrt{2\pi}} \exp(-t^2/2) \leq \pr{g \geq t} \leq \frac{1}{t} \cdot \frac{1}{\sqrt{2 \pi}} \exp(-t^2/2). \]
In particular, for \(t \geq 1\) the tail is bounded by the density:
\[ \pr{g \geq t} \leq \frac{1}{\sqrt{2\pi}} \exp(-t^2/2). \]</p>
</div>
<div class="proof">
  <div></div>
<p>To obtain upper bound on the tail \(\pr{g \geq t}\), we apply the change of variables \(x - t \to y\)
\[ \pr{g \geq t} = \frac{1}{\sqrt{2\pi}} \int_0^{\infty} e^{-t^2/2} e^{-ty} e^{-y^2/2} dy \leq \frac{1}{\sqrt{2\pi}} e^{-t^2/2} \int_0^{\infty} e^{-ty} dy, \]
where we used that \(e^{-y^2/2} \leq 1\) for any \(y\). Since the last integrasl equals \(1/t\), we have proved the upper bound.</p>
<p>The lower bound on the tail follows from the identity
\[ \int_0^{\infty} (1-3x^{-4}) e^{-x^2/2} dx = \br{\frac{1}{t} - \frac{1}{t^3}} e^{-t^2/2}. \]
This completes the proof.</p>
</div>
<div class="theorem" id="thm:berry-essen-central-limit">
  <div></div>
<p>In the setting of Theorem <a href="#thm:lindeberg-levy-central-limit-theorem">thm:lindeberg-levy-central-limit-theorem</a>, for every \(N\) and every \(t \in \R\) we have
\[ \abs{\pr{Z_N \geq t} - \pr{g \geq t}} \leq \frac{\rho}{\sqrt{N}}. \]
Here \(\rho \eqdef \ec{\frac{\abs{X_1 - \mu}^3}{\sigma^3}}\) and \(g \simeq N(0, 1)\).</p>
</div>
<div class="proof">
  <div></div>
<p>TODO.</p>
</div>
<div class="proposition" id="prop:stirlings-formula">
  <div></div>
<p>Stirling&rsquo;s approximation states that
\[ n! \simeq \sqrt{2\pi n} \br{\frac{n}{e}}^{n}. \]</p>
</div>
<div class="proof">
  <div></div>
<p>TODO.</p>
</div>
<div class="exercise">
  <div></div>
<p>(Truncated normal distribution). Let \(g \sim N(0, 1)\). Show that for all \(t \geq 1\), we have
\[ \ec{g^2 1_{g &gt; t}} = t \cdot \frac{1}{\sqrt{2 \pi}} \exp(-\frac{t^2}{2}) + \pr{g \geq t} \leq \br{t + \frac{1}{t}} \frac{1}{\sqrt{2 \pi }} \exp \br{\frac{-t^2}{2}}. \]</p>
</div>
<div class="proof">
  <div></div>
<p>By direct evaluation and using integration by parts we have</p>
<p>\begin{align}
\ec{g^2 1_{g &gt; t}} &amp;= \int_{\tau = t}^{\infty} \frac{\tau^{2}}{\sqrt{2\pi}} d \tau \\\<br>
&amp;= \frac{-1}{\sqrt{2\pi}} \int_{\tau=t}^{\infty} (\tau) \left [ \exp\br{-\frac{\tau^2}{2} (-\tau)} \right ] d\tau \\\<br>
&amp;= \frac{-1}{\sqrt{2\pi}} \left [ (\tau \exp\br{-\frac{\tau^2}{2}})_{\tau=t}^{infty} - \int_{\tau=t}^{\infty} \exp\br{-\frac{\tau^2}{2}} d \tau \right ]. \\\<br>
&amp;= \frac{-1}{\sqrt{2 \pi }} \left [ - t \exp\br{\frac{-t^2}{2}} - \sqrt{2 \pi } \pr{g \geq t} \right ] \\\<br>
&amp;= \frac{t}{\sqrt{2 \pi }} \exp\br{\frac{-t^2}{2}} + \pr{g \geq t}.
\end{align}</p>
<p>The other part of the inequality follows trivially by Proposition <a href="#prop:tails-of-normal-distribution">prop:tails-of-normal-distribution</a>.</p>
</div>
<h3 id="2-dot-2-hoeffding-s-inequality">2.2: Hoeffding&rsquo;s inequality</h3>
<div class="definition" id="def:symmetric-bernoulli-distribution">
  <div></div>
<p>A random variable \(X\) has symmetric Bernoullli distribution if it takes values \(1\) and \(-1\)  with probabilities \(\frac{1}{2}\) each, i.e.
\[ \pr{X=-1} = \pr{X=1} = \frac{1}{2}. \]</p>
</div>
<div class="theorem" id="thm:hoeffding-inequality">
  <div></div>
<p>Let \(X_1, X_2, \ldots, X_N\) be independent symmetric Bernoulli random variables, and \(a = (a_1, \ldots, a_N) \in \R^N\). Then, for any \(t \geq 0\), we have
\[ \pr{\sum_{i=1}^N a_i X_i \geq t} \leq \exp\br{-\frac{t^2}{2\sqn{a}_2}}. \]</p>
</div>
<div class="proof">
  <div></div>
<p>We assume without loss of generality that \(\norm{a}_2 = 1\). Using Markov&rsquo;s Inequality (Proposition <a href="#prop:markov-inequality">prop:markov-inequality</a>), we have</p>
<p>\begin{align}
\label{eq:hoeffding-proof-0}
\pr{\sum_{i=1}^N a_i X_i \geq t} &amp;= \pr{\exp\br{\lambda \sum_{i=1}^N a_i X_i } \geq \exp\br{\lambda t}} \\\<br>
&amp;\leq \exp(-\lambda t) \ec{\exp\br{\lambda \sum_{i=1}^N a_i X_i}}.
\end{align}</p>
<p>Hence we have reduced the problem to bounding the moment generating function (MGF) of the sum \(\sum_{i=1}^N a_i X_i\). The MGF of the sum of <strong>independent</strong> random variable is the product of the MGF of the terms: hence,</p>
<p>\begin{equation}
\label{eq:hoeffding-proof-1}
\ec{\exp\br{\lambda \sum_{i=1}^N a_i X_i}} = \prod_{i=1}^N \ec{\exp\br{\lambda a_i X_i}}.
\end{equation}</p>
<p>For a fixed \(i\), we have</p>
<p>\begin{equation}
\label{eq:hoeffding-proof-2}
\ec{\exp\br{\lambda a_i X_i}} = \frac{\exp(\lambda a_i) + \exp(-\lambda a_i)}{2} = \cosh(\lambda a_i).
\end{equation}</p>
<p>Using Exercise <a href="#exercise-2-2-3">exercise-2-2-3</a> and the last inequality, we have
\[ \ec{\exp\br{\lambda a_i X_i}} \leq \exp\br{\lambda^2 a_i^2 / 2}. \]
Plugging this back into \eqref{eq:hoeffding-proof-1} and \eqref{eq:hoeffding-proof-0}, we get</p>
<p>\begin{align}
\pr{\sum_{i=1}^N a_i X_i \geq t} &amp;\leq \exp(-\lambda t) \prod_{i=1}^N \exp\br{\lambda^2 a_i^2 / 2} = \exp\br{-\lambda t + \frac{\lambda^2}{2} \sum_{i=1}^N a_i^2} \\\<br>
&amp;= \exp\br{-\lambda t + \frac{\lambda^2}{2}}.
\end{align}</p>
<p>In the last identity, we used the assumption that \(\norm{a}_2 = 1\). Maximizing over \(\lambda\) gives
\[ \pr{\sum_{i=1}^N a_iX_i \geq t} \leq \exp\br{-t^2/2}. \]
This completes the proof of Hoeffding&rsquo;s inequality.</p>
</div>
<div class="exercise" id="exercise-2-2-3">
  <div></div>
<p>(Bounding the hyperbolic cosine). Show that
\[ \cosh(x) \leq \exp(x^2/2). \]
Hint: compare the Taylor&rsquo;s expansion of both sides.</p>
</div>
<div class="proof">
  <div></div>
<p>TODO.</p>
</div>
<div class="lemma" id="lemma:hoeffding-lemma">
  <div></div>
<p>Suppose that \(Z\) is a zero-mean random variable such that \(Z \leq [a, b]\) almost surely. Define</p>
<p>\begin{equation}
\label{eq:hoeffding-lemma-0}
\psi_Z(\lambda) = \log \ec{\exp\br{\lambda Z}}.
\end{equation}</p>
<p>Then we have, \(\psi_Z^{\prime\prime} (\lambda) \leq \frac{(b-a)^2}{4}\). A consequence of this is</p>
<p>\begin{equation}
\label{eq:hoeffding-lemma}
\psi_Z(\lambda) \leq \frac{\lambda^2 (b-a)^2}{8}.
\end{equation}</p>
</div>
<div class="proof">
  <div></div>
<p>By directly differentiating \(\psi\) with respect to \(\lambda\) we have,</p>
<p>\begin{align}
\psi_Z^{\prime}(\lambda) &amp;= \frac{\ec{Z\exp\br{\lambda Z}}}{\ec{\exp\br{\lambda Z}}}, \text { and, } \ \psi_Z^{\prime \prime}(\lambda) &amp;= \frac{\ec{Z^2 \exp(\lambda Z)}}{\ec{\exp(\lambda Z)}} - \br{\frac{\ec{Z \exp(\lambda Z)}}{\ec{\exp(\lambda Z)}}}^2.
\end{align}</p>
<p>Note that by a change of measure argument \(\pr{z} \to \frac{\exp(\lambda z) \pr{z}}{\ec{\exp(\lambda z)}}\), we can interpret \(\psi_Z^{\prime\prime}\) as the variance of the random variable \(Z\) under the new measure. Since \(Z\) is still bounded in \([a, b]\), this variance is at most \(\frac{(b-a)^2}{4}\), completing the proof of \eqref{eq:hoeffding-lemma-0}. To get \eqref{eq:hoeffding-lemma}, simply integrate twice.</p>
</div>
<div class="theorem" id="thm:hoeffding-inequality-general">
  <div></div>
<p>Let \(X_1, \ldots, X_N\) be independent random variables. Assume that \(X_i \in [m_i, M_i]\) for every \(i\). Then, for any \(t &gt; 0\), we have
\[ \pr{\sum_{i=1}^{N} \br{X_i - \ec{X_i}} \geq t} \leq \exp\br{-\frac{2t^2}{\sum_{i=1}^N (M_i - m_i)^2}}. \]</p>
</div>
<div class="proof">
  <div></div>
<p>We do everything as in the proof of Theorem <a href="#thm:hoeffding-inequality">thm:hoeffding-inequality</a> but using Lemma <a href="#lemma:hoeffding-lemma">lemma:hoeffding-lemma</a> instead of explicitly computing \(\ec{\exp\br{\lambda (X_i - \ec{X_i})}}\). It is straightforward.</p>
</div>
<div class="exercise">
  <div></div>
<p>Prove Theorem <a href="#thm:hoeffding-inequality-general">thm:hoeffding-inequality-general</a>, possibly with some absolute constant instead of \(2\) in the tail.</p>
</div>
<div class="proof">
  <div></div>
<p>See the proof of Theorem <a href="#thm:hoeffding-inequality-general">thm:hoeffding-inequality-general</a>.</p>
</div>
<div class="exercise">
  <div></div>
<p>Given an algorithm that makes a binary decision at random and returns the correct answer with probability \(\frac{1}{2}+\delta\) for some \(\de &gt; 0\). To improve the algorithm&rsquo;s performance, we run it \(N\) times and take the majority vote. Show that for any \(\epsilon \in (0, 1)\) the answer is correct with probability at least \(1 - \epsilon\), as long as</p>
<p>\begin{equation}
\label{eq:exc-2-2-8-result}
N \geq \frac{1}{2 \delta^{2}} \ln \br{\frac{1}{\epsilon}}.
\end{equation}</p>
</div>
<div class="proof">
  <div></div>
<p>Let \(Z_i\) represent the decision at the ith invocation of the algorithm. Let \(X_i\) represent whether the decision taken by the algorithm is incorrect (\(X_i = 1\)) or correct (\(X_i = 0\)) in the i-th trial. We have</p>
<p>\begin{equation}
\label{eq:exc-2-2-8-1}
\ec{X_i} = 1 \cdot (1 - \br{\frac{1}{2} + \delta}) + 0 = \frac{1}{2} - \delta.
\end{equation}</p>
<p>Note that the majority vote here corresponds to
\[ M = \begin{cases}
1 &amp; \text { if } \frac{1}{N}\sum_{i=0}^N Z_i &gt; \frac{1}{2}, \\\<br>
0&amp; \text { otherwise. }
\end{cases}
\]
The probability that \(M\) is the wrong decision is the probability
\[ \pr{\sum_{i=0}^N X_i &gt; \frac{N}{2}}. \]
We can use Hoeffding&rsquo;s inequality (Theorem <a href="#thm:hoeffding-inequality-general">thm:hoeffding-inequality-general</a>) to bound this probability as the variables \(X_i \in [0, 1]\):
\[ \pr{\sum_{i=1}^N (X_i - \frac{1}{2} + \delta) \geq t} \leq \exp\br{-\frac{2t^2}{N}}. \]
Equivalently,
\[ \pr{\sum_{i=1}^N X_i \geq t} \leq \exp\br{\frac{-2\br{t + N\delta - N/2}^2}{N}}. \]
Put \(t = N/2\), then
\[ \pr{\sum_{i=1}^N X_i \geq \frac{N}{2}} \leq \exp\br{-2 \delta^2 N}. \]
Setting the right hand side to be smaller than \(\epsilon\) yields \eqref{eq:exc-2-2-8-result}.</p>
</div>
<div class="exercise">
  <div></div>
<p>(Robust estimation of the mean). Suppose we want to estimate the mean \(\mu\) of a random variable \(X\) from a sample \(X_1, \ldots, X_N\) drawn i.i.d. from the distribution of \(X\). We want an \(\e\) accurate estimate: one that falls in \((\mu - \e, \mu + \e)\).</p>
<ul>
<li>Show that a sample of size \(N = \cO\br{\sigma^2/\epsilon^2}\) is sufficient to compute an \(\e\) accurate estimate with probability at least \(3/4\), where \(\sigma^2 = \Var(X)\).</li>
<li>Show that a sample of size \(N = \cO\br{\log(\delta^{-1}) \sigma^2/\e^2}\) is sufficient to compute an \(\epsilon\) accurate estimate with probability at least \(1 - \delta\).</li>
</ul>
</div>
<div class="proof">
  <div></div>
<ul>
<li>We start by using Chebyshev&rsquo;s inequality (Corollary <a href="#prop:chebychev-inequality">prop:chebychev-inequality</a>) on the sample mean:</li>
</ul>
<p>\[ \pr{\abs{\frac{1}{n} \sum_{i=1}^{n} X_{i}  - \mu} \geq t} \leq \frac{\sigma^2}{N t^2}, \]
where we used that the variance of the sample mean of independent random variables is reduced by a factor of \(N\). Putting \(t = \epsilon\) and choosing \(N=\frac{4 \sigma^2}{\e^2}\) yields the first requirement.</p>
<ul>
<li>FIXME.</li>
</ul>
</div>
<div class="exercise" id="exr-2-2-10">
  <div></div>
<p>(Small ball probabilities). Let \(X_1, \ldots, X_N\) be non-negative independent random variables with continuous distributions. Assume that the densities of \(X_i\) are uniformly bounded by \(1\).</p>
<ol>
<li>
<p>Show that the MGF of \(X_i\) satisfies</p>
<p>\begin{equation}
\label{eq:exr-2-2-10-a}
\ec{\exp(-tX_i)} \leq \frac{1}{t}, \text { for all } t &gt; 0.
\end{equation}</p>
</li>
<li>
<p>Deduce that, for any \(\e &gt; 0\), we have</p>
<p>\begin{equation}
\label{eq:exr-2-2-10-b}
\pr{\sum_{i=1}^N X_i \leq \e N} \leq (e \e)^N.
\end{equation}</p>
</li>
</ol>
</div>
<div class="proof">
  <div></div>
<ol>
<li>
<p>We have</p>
<p>\begin{align}
\ec{\exp(-tX_i)} &amp;= \int_{x=0}^{\infty} \exp(-t x) p(x) dx \\\<br>
&amp;\leq \int_{x=0}^{\infty} \exp(-tx) dx \\\<br>
&amp;= \left [ \frac{\exp(-tx)}{-t} \right ]_{x=0}^{\infty} \\\<br>
&amp;= \frac{1}{t}.
\end{align}</p>
</li>
<li>
<p>We transform the inequality of the sum to a product of the MGFs using Markov&rsquo;s Inequality (Proposition <a href="#prop:markov-inequality">prop:markov-inequality</a>) and then use \eqref{eq:exr-2-2-10-a}:</p>
<p>\begin{align}
\pr{\sum_{i=1}^N X_i \leq \e N} &amp;= \pr{\exp\br{\sum_{i=1}^N -t X_i} \geq -t \e N} \\\<br>
&amp;= \leq \frac{ \ec{\exp\br{-t\sum_{i=1}^N X_i}} }{\exp(-t \e N)} \\\<br>
&amp;= \frac{\prod_{i=1}^{N} \ec{\exp(-t X_i)}}{\exp(-t \e N)} \\\<br>
&amp;\leq \br{\frac{1}{t}}^N \exp(t \e N).
\end{align}</p>
</li>
</ol>
<p>Choosing \(t = \frac{1}{\e}\) yields \eqref{eq:exr-2-2-10-b}.</p>
</div>
<h3 id="2-dot-3-chernoff-s-inequality">2.3: Chernoff&rsquo;s inequality</h3>
<div class="theorem" id="thm:chernoff-inequality">
  <div></div>
<p>Let \(X_i\) be independent Bernoulli random variables with parameters \(p_i\). Consider the sum \(S_N = \sum_{i=1}^{N}{X_i}\) and denote its mean by \(\mu = \ec{S_N}\). Then, for any \(t \geq \mu\), we have</p>
<p>\begin{equation}
\label{eq:chernoff-inequality-1}
\pr{S_N \geq t} \leq \exp(-\mu) \br{\frac{e\mu}{t}}^t.
\end{equation}</p>
</div>
<div class="proof">
  <div></div>
<p>Going as in the proof of Theorem <a href="#thm:hoeffding-inequality">thm:hoeffding-inequality</a>, we obtain
\[ \pr{S_N \geq t} \leq \exp(-\lambda t) \prod_{i=1}^{N} \ec{\exp\br{\lambda X_i}}.
\]
By direct computation we have,</p>
<p>\begin{equation}
\ec{\exp\br{\lambda X_i}} = p_i \exp\br{\lambda} + (1-p_i) = p_i \br{\exp\br{\lambda} - 1} + 1 \leq \exp\br{\br{e^{\lambda} - 1} p_i}.
\end{equation}</p>
<p>In the last step, we used the numeric inequality \(1 + x \leq e^x\). Consequently,</p>
<p>\begin{align}
\prod_{i=1}^{N} \ec{\exp\br{\lambda X_i}} &amp;\leq \prod_{i=1}^{N} \exp\br{\br{e^{\lambda} - 1} p_i}
&amp;\leq \exp\br{ \br{e^{\lambda} - 1} \sum_{i=1}^{N} p_{i}  }.
&amp;= \exp\br{ \br{e^{\lambda} - 1} \mu }.
\end{align}</p>
<p>Putting \(\lambda = \ln \br{t/\mu}\) which is positive by \(t &gt; \mu\) and simplifying yields \eqref{eq:chernoff-inequality-1}.</p>
</div>
<div class="exercise" id="exercise2p3p2p">
  <div></div>
<p>(Chernoff&rsquo;s inequality: lower tails). Modify the proof of Theorem <a href="#thm:chernoff-inequality">thm:chernoff-inequality</a> to obtain the following lower tail. For any \(t &lt; \mu\), we have
\[ \pr{S_N \leq t} \leq \exp(-\mu) \br{\frac{e \mu}{t}}^t. \]</p>
</div>
<div class="proof">
  <div></div>
<p>We proceed similarly to the original Chernoff inequality proof, using Markov&rsquo;s inequality (Proposition <a href="#prop:markov-inequality">prop:markov-inequality</a>):</p>
<p>\begin{align}
\pr{S_{N} \leq t} &amp;= \pr{\sum_{i=1}^{N} X_i \leq t} \\\<br>
&amp;= \pr{\exp(- \lambda \sum_{i=1}^N X_i) \geq \exp\br{-\lambda t} } \\\<br>
\label{eq:exr-2-3-2p0}
&amp;\leq \frac{ \prod_{i=1}^{N} \ec{\exp\br{-\lambda X_i}}}{\exp(-\lambda t)}.
\end{align}</p>
<p>We can explicitly compute the expectation in the last line when \(X_i\) is Bernoulli:
\[ \ec{\exp\br{-\lambda_i X_i}} = p_i \exp\br{-\lambda} + (1-p_i) = p_i (\exp\br{-\lambda} - 1) + 1 \leq \exp\br{p_i \br{\exp\br{-\lambda} + 1}}. \]
In the last step, we used the fact that \(1 + x \leq \exp(x)\) for all \(x \in \R\). Plugging this back into \eqref{eq:exr-2-3-2p0}, we obtain</p>
<p>\begin{align}
\pr{S_N \leq t} &amp;\leq \frac{\prod_{i=1}^{N} \exp\br{p_i \br{\exp\br{-\lambda} - 1}}}{\exp\br{-\lambda t}} \\\<br>
&amp;= \frac{\exp{\br{\sum_{i=1}^{N}{p_i \br{\exp{\br{-\lambda}} - 1}}}}}{\exp{\br{-\lambda t}}} \\\<br>
&amp;= \frac{\exp{\br{\mu \br{\exp\br{-\lambda} - 1}}}}{\exp\br{-\lambda t}}.
\end{align}</p>
<p>Putting \(t = \ln \br{\mu/t}\) and simplifying yields the required inequality.</p>
</div>
<div class="exercise">
  <div></div>
<p>(Poisson tails). Let \(X \sim \Pois(\lambda)\). Show that for any \(t &gt; \lambda\), we have
\[ \pr{X &gt; t} \leq \exp\br{-\lambda} \br{\frac{e \lambda}{t}}^t. \]</p>
</div>
<div class="proof">
  <div></div>
<p>For every \(N \in \N\), define \(p_{N, 1}, p_{N, 2}, \ldots, p_{N, N}\) by</p>
<p>\begin{equation}
\label{eq:exc-2-3-3-0}
p_{N, i} \eqdef \frac{\lambda}{N}.
\end{equation}</p>
<p>Then \(\max_{i\leq N} p_{N, i} \to 0\) as \(N \to \infty\) and \(\sum_{i=1}^N P_{N, i} = \lambda\) for all \(N\). Define \(X_{N, i}\) as a random variable with distribution \(\Bern(p_{N,i})\). Note that we have by Theorem <a href="#thm:chernoff-inequality">thm:chernoff-inequality</a> that</p>
<p>\begin{equation}
\pr{S_N \geq t} \leq \exp\br{-\lambda} \br{\frac{e\lambda}{t}}^t,
\end{equation}</p>
<p>where \(S_N = \sum_{i=1}^N X_{N, i}\). By Theorem <a href="#thm:poisson-limit-theorem">thm:poisson-limit-theorem</a>, we can identify the random variable \(X\) as \(\lim_{N\to\infty} S_N\). Taking the limit of as \(N \to \infty\) yields
\[ \pr{X \geq t} \leq \exp\br{-\lambda} \br{\frac{e\lambda}{t}}^t,     \]
as required.</p>
</div>
<div class="exercise">
  <div></div>
<p>Show that, in the setting of Theorem <a href="#thm:chernoff-inequality">thm:chernoff-inequality</a>, for \(\delta \in (0, 1]\), we have
\[ \pr{\abs{S_N - \mu} \geq \delta \mu} \leq 2 \exp\br{-c\mu\delta^2} \]
where \(c &gt; 0\) is an absolute constant.</p>
</div>
<div class="proof">
  <div></div>
<p>TODO.</p>
</div>
<div class="exercise">
  <div></div>
<p>TODO.</p>
</div>
<div class="exercise">
  <div></div>
<p>TODO.</p>
</div>
<h3 id="2-dot-4-applications-degrees-of-random-graphs-dot">2.4 Applications: degrees of random graphs.</h3>
<div class="definition" id="def:erdös-rényi-model">
  <div></div>
<p>The Erdös-Rényi model is a random graph \(G(n, p)\) constructed on a set of \(n\) vertices by connecting every pair of distinct vertices independently with probability \(p\). We define the <em>degree</em> of a vertex in the graph as the number of edges incident to that vertex. The expected degree of every vertex in \(G(n, p)\) equals \(d \eqdef (n-1)p\).</p>
</div>
<div class="proposition" id="prop:dense-graphs-are-almost-regular">
  <div></div>
<p>There is an absolute constant \(C\) such th at the following holds. Consider a random graph \(G \sim G(n, p)\) with expected degree satisfying \(d \geq C \log n\). Then, with high probability (for example \(0.9\)) the following occurs: all vertices of \(G\) have degrees between \(0.9d\) and \(1.1d\).</p>
</div>
<div class="proof">
  <div></div>
<p>The degree of node \(i\), denoted \(d_i\), is a sum of \(n-1\) independent \(\Bern(p)\) random variables. Thus, applying Chernoff&rsquo;s inequality (Theorem <a href="#thm:chernoff-inequality">thm:chernoff-inequality</a>) we obtain that for some \(c\) it holds
\[ \pr{\abs{d_i  - d} \geq 0.1d} \leq 2 \exp\br{-cd}. \]
This bound holds for any fixed \(i\), by taking the union bound over all the \(n\) vertices we obtain
\[ \pr{\exists i \leq n : \abs{d_i - d} \geq 0.1d} \leq \sum_{i=1}^{n} \pr{\abs{d_i - d} \geq 0.1d} \leq n \cdot 2 \exp\br{-cd}. \]
It remains to use the assumed bound on \(d\).</p>
</div>
<div class="exercise">
  <div></div>
<p>(Bounding the degree of sparse graphs). Consider a random graph \(G \sim G(n, p)\) with expected degrees \(d = o \br{\log(n)}\). Show that with high probability, all vertices of \(G\) have degrees \(\cO\br{\log n}\).</p>
</div>
<div class="proof">
  <div></div>
</div>
<h2 id="bibliography">Bibliography</h2>
<p><a id="citeproc_bib_item_1"></a>Vershynin, Roman. 2018. “High-Dimensional Probability: An Introduction with Applications in Data Science.” Cambridge University Press. <a href="https://doi.org/10.1017/9781108231596">https://doi.org/10.1017/9781108231596</a>.</p>



        <div class="footer">
    Powered by <a href="https://gohugo.io/">Hugo</a> with
    <a href="https://github.com/mrmierzejewski/hugo-theme-console/">Console Theme</a>. 
</div>

    </div>
  </body>
</html>
