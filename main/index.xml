<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mains on Ahmed&#39;s corner</title>
    <link>https://rka97.github.io/main/</link>
    <description>Recent content in Mains on Ahmed&#39;s corner</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://rka97.github.io/main/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>About</title>
      <link>https://rka97.github.io/main/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://rka97.github.io/main/about/</guid>
      <description>akregeb at gmail dot com
Welcome to my tiny corner of the internet! I&amp;rsquo;m Ahmed, I work on optimization and machine learning. I have a Bachelor&amp;rsquo;s degree in Computer Engineering from Cairo University, Egypt. I&amp;rsquo;m (hopefully) going to join Princeton&amp;rsquo;s ECE department as a Ph.D. student starting next year.
I was fortunate to intern in the group of Prof. Peter Richtárik at KAUST in the summers of 2019/2020, where I worked on the distributed &amp;amp; stochastic optimization.</description>
    </item>
    
    <item>
      <title>Papers</title>
      <link>https://rka97.github.io/main/papers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://rka97.github.io/main/papers/</guid>
      <description>(In reverse order of preparation)
 Proximal and Federated Random Reshuffling Preprint (2021), with Konstantin Mishchenko and Peter Richtárik. (bibtex). Unified Analysis of Stochastic Gradient Methods for Composite Convex and Smooth Optimization Preprint (2020), with Othmane Sebbouh, Nicolas Loizou, Robert M. Gower, and Peter Richtárik. (bibtex). Random Reshuffling: Simple Analysis with Vast Improvements Advances in Neural Information Processing Systems 33 (NeurIPS 2020), with Konstantin Mishchenko and Peter Richtárik. (bibtex). Better Theory for SGD in the Nonconvex World Preprint (2020), with Peter Richtárik.</description>
    </item>
    
    <item>
      <title>Talks</title>
      <link>https://rka97.github.io/main/talks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://rka97.github.io/main/talks/</guid>
      <description> On the Convergence of Local SGD on Identical and Heterogeneous Data Federated Learning One World Seminar (2020). Video and Slides.  </description>
    </item>
    
  </channel>
</rss>
